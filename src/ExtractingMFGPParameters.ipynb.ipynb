{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b6e88c-03b6-4140-a7ea-c49d24d8f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sutirthapaul/miniconda3/envs/benzenetest2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch as gpt\n",
    "import botorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP, SingleTaskGP\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.fit import fit_gpytorch_mll, fit_gpytorch_mll_torch\n",
    "from torch.optim import Adam\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import HeBz\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Use CPU for this example\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce70d47-60e6-403b-b96e-178e97983b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.61939766 3.46410162 3.47733297 ... 4.33012702 4.5        4.        ]\n"
     ]
    }
   ],
   "source": [
    "#Load the data from file\n",
    "V = np.load('../data/HeBz_sector_2475.npy')\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "z = np.array([])\n",
    "data_points = []\n",
    "Pot = np.array([])\n",
    "Pot_diff = np.array([])\n",
    "for array in V:\n",
    "    x = np.append(x,array[0])\n",
    "    y = np.append(y,array[1])\n",
    "    z = np.append(z,array[2])\n",
    "    data_points.append([array[0],array[1],array[2],1.0])\n",
    "    Pot = np.append(Pot,array[3])\n",
    "V2 = np.load('../data/new_pts_70.npy')\n",
    "for array in V2:\n",
    "    x = np.append(x,array[0])\n",
    "    y = np.append(y,array[1])\n",
    "    z = np.append(z,array[2])\n",
    "    data_points.append([array[0],array[1],array[2],1.0])\n",
    "    Pot = np.append(Pot,array[3])\n",
    "data_points = np.array(data_points)\n",
    "noise = 1e-6*np.ones_like(Pot)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45cc6cc-cbfd-416f-9b6f-c63d90ad8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47\n",
      " 49 51 53 55 57 59 61 63 65 67 69 71 73]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(1,75,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936698a9-75e3-4e1e-a2ae-858034fdce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "LJvec = HeBz.LennardJones\n",
    "Shirvec = HeBz.Shirkov2024\n",
    "Leevec = HeBz.Lee2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dc16ed-f1fb-4113-8236-2a2ebc0e352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  9.45945946e-02 ...  1.21158616e-08\n",
      "  -1.83603042e+04  1.00634447e+05]\n",
      " [ 0.00000000e+00  0.00000000e+00  8.51351351e-01 ... -2.11424265e-09\n",
      "  -8.68586691e+04  4.83971583e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.60810811e+00 ...  3.99535650e-10\n",
      "  -2.31209156e+04  7.70900596e+03]\n",
      " ...\n",
      " [ 7.00000000e+00  0.00000000e+00  5.39189189e+00 ...  1.37187109e-11\n",
      "   4.48182311e-01  2.05744395e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00  6.14864865e+00 ... -3.75382973e-11\n",
      "  -2.01439312e-01  2.36286681e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00  6.90540541e+00 ...  9.38433445e-13\n",
      "   9.39158574e-02  2.36453566e+00]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Generate Source Task Data\n",
    "# -----------------------------\n",
    "DFTdata0 = np.load(\"../data/pbe0_113850_CP_D4_processed.npy\")\n",
    "ind = np.where(np.isclose(DFTdata0[:,2],0.09459459))\n",
    "refdata = DFTdata0[ind]\n",
    "refdata[:,2] = -1*refdata[:,2]\n",
    "DFTdata = np.concatenate((DFTdata0,refdata))\n",
    "ind = np.argsort(DFTdata[:,2], kind='stable')\n",
    "DFTdatazsrt = DFTdata[ind]\n",
    "ind = np.argsort(DFTdatazsrt[:,1], kind='stable')\n",
    "DFTdatayzsrt = DFTdatazsrt[ind]\n",
    "DFTdataxrem = DFTdatayzsrt[::8]\n",
    "ind = np.argsort(DFTdataxrem[:,0], kind='stable')\n",
    "DFTdataxremzsrt = DFTdataxrem[ind]\n",
    "DFTdatazxrem = DFTdataxremzsrt\n",
    "print(DFTdatazxrem)\n",
    "DFTdata2 = np.load(\"../data/pbe0_corr_sutirtha_CP_D4_full_processed.npy\")\n",
    "#print(DFTdatazxrem)\n",
    "#print(np.min(DFTdata2[:,-1]))\n",
    "DFTdatanew = np.concatenate((DFTdata2[:,0],DFTdata2[:,1],DFTdata2[:,2]),)\n",
    "DFTtotal = np.concatenate((DFTdatazxrem[:,0:3],DFTdata2[:,0:3]))\n",
    "data_source = [[x, y, z, w] for x, y, z, w in zip(DFTtotal[:,0],DFTtotal[:,1],DFTtotal[:,2],np.zeros(len(DFTtotal)))]\n",
    "m_source = np.concatenate((DFTdatazxrem[:,-1],DFTdata2[:,-1]))\n",
    "noise_source = 1e-6*np.ones_like(m_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffda53d-4541-4387-9d56-1d90fddbabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840\n"
     ]
    }
   ],
   "source": [
    "ind = DFTtotal[:,0] > 4.4\n",
    "xnew = np.unique(DFTtotal[:,0][ind])\n",
    "ynew = np.unique(DFTtotal[:,1][ind])\n",
    "print(len(xnew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38beadf1-2f84-4608-98a3-adcddeb6d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([14425, 16654, 16682, 16687, 16696, 16700, 16708, 16712, 16724,\n",
      "       16730, 16740, 16914, 16940, 16960]),)\n",
      "[4.12 3.   1.5  0.9  5.   1.8  2.1  7.   2.7  2.4  1.2  0.6  0.3  0.  ]\n"
     ]
    }
   ],
   "source": [
    "ind = np.where(np.isclose(DFTtotal[:,0],4.5))\n",
    "print(ind)\n",
    "xpl2 = DFTtotal[:,0][ind]\n",
    "ypl2 = DFTtotal[:,1][ind]\n",
    "zpl2 = DFTtotal[:,2][ind]\n",
    "Potpl2 = m_source[ind]\n",
    "print(zpl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0579e5-dc47-479d-91d4-dd8eba102d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19506, 4)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.concatenate((data_points,data_source))\n",
    "train_Y = np.concatenate((Pot,m_source))\n",
    "train_Y = train_Y.reshape(-1,1)\n",
    "train_Yvar = np.concatenate((noise,noise_source))\n",
    "train_Yvar = train_Yvar.reshape(-1,1)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f6faf2-f98a-4424-af2b-76d1c40cd3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18162, 4)\n"
     ]
    }
   ],
   "source": [
    "train_Y_trunc = train_Y[train_Y[:, 0]<=1000]\n",
    "train_Yvar_trunc = train_Yvar[train_Y[:,0]<=1000]\n",
    "train_X_trunc = train_X[train_Y[:, 0]<=1000, :]\n",
    "print(train_X_trunc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a885826c-0230-411c-a68b-df554f023219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sutirthapaul/miniconda3/envs/benzenetest2/lib/python3.11/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning: Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('../data/my_model_multfidDFT16kwNoiseRefBetter.pth')\n",
    "model = SingleTaskMultiFidelityGP(torch.from_numpy(train_X_trunc), torch.from_numpy(train_Y_trunc), torch.from_numpy(train_Yvar_trunc), data_fidelities=[len(train_X_trunc[0]) - 1], input_transform=Normalize(d=4),outcome_transform=Standardize(m=1))\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "#Manually remap keys for version compatibility\n",
    "\"\"\"\n",
    "new_state_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    \n",
    "    if (k == \"likelihood.noise_covar.noise_prior._transformed_loc\"):\n",
    "        new_state_dict[\"likelihood.noise_covar.noise_prior.concentration\"] = v\n",
    "    elif (k == \"likelihood.noise_covar.noise_prior._transformed_scale\"):\n",
    "        new_state_dict[\"likelihood.noise_covar.noise_prior.rate\"] = v\n",
    "    elif (k == \"outcome_transform._is_trained\"):\n",
    "        continue\n",
    "    else:\n",
    "        new_key = k.replace(\"kernels.0.\", \"\")  \n",
    "        new_state_dict[new_key] = v\n",
    "\"\"\"\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b1d8a11-6dc3-4535-b825-0ea796808493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_kernel(x1, x2, lengthscale):\n",
    "    \"\"\"\n",
    "    Compute Matern kernel between x1 and x2 with given lengthscale and smoothness nu.\n",
    "    \"\"\"\n",
    "    # pairwise distance\n",
    "    r = torch.cdist(x1 / lengthscale, x2 / lengthscale, p=2)\n",
    "    #print(r)\n",
    "    sqrt5 = torch.sqrt(torch.tensor(5.0))\n",
    "    k = (1 + sqrt5 * r + 5 * r**2 / 3) * torch.exp(-sqrt5 * r)\n",
    "    \n",
    "    return k\n",
    "def kernel(X1,X2,theta1,theta2,P,scale):\n",
    "    # split x and z\n",
    "    x1, z1 = X1[..., :-1], X1[..., -1:]\n",
    "    x2, z2 = X2[..., :-1], X2[..., -1:]\n",
    "    x11_ = z1\n",
    "    x21t = z2\n",
    "    x21t_ = x21t.transpose(-1, -2)\n",
    "    cross_term_1 = (1 - x11_) * (1 - x21t_)\n",
    "    bias_factor = cross_term_1 * (1 + x11_ * x21t_).pow(P)\n",
    "    return scale*(matern_kernel(x1,x2,lengthscale=theta1) + bias_factor*matern_kernel(x1,x2,lengthscale=theta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d835e1-8aa6-458f-8c36-224db82804b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6599, 0.5467, 0.5941, 1.0000],\n",
      "        [0.4949, 0.5714, 0.5941, 1.0000],\n",
      "        [0.4968, 0.2662, 0.5941, 1.0000],\n",
      "        ...,\n",
      "        [0.5939, 0.4920, 0.8647, 0.0000],\n",
      "        [0.5143, 0.0000, 0.8647, 0.0000],\n",
      "        [0.6429, 0.0000, 0.0133, 0.0000]], dtype=torch.float64)\n",
      "[[-2.70225982e+00]\n",
      " [ 9.57649686e+00]\n",
      " [ 2.78542325e+00]\n",
      " ...\n",
      " [ 1.73981852e-01]\n",
      " [ 1.75306113e+00]\n",
      " [-2.07882331e+02]]\n",
      "{'outputscale': array(814.69663559), 'theta1': array([[0.78638807, 2.17270815, 0.77220716]]), 'theta2': array([[2.03248109, 5.05874827, 1.71032378]]), 'learned_mean': array(22.0553313), 'power': array([0.63911297]), 'data': array([[0.65991395, 0.54669062, 0.59405714, 1.        ],\n",
      "       [0.49487166, 0.57142857, 0.59405714, 1.        ],\n",
      "       [0.49676185, 0.26621387, 0.59405714, 1.        ],\n",
      "       ...,\n",
      "       [0.59392256, 0.49202156, 0.86468571, 0.        ],\n",
      "       [0.51428571, 0.        , 0.86468571, 0.        ],\n",
      "       [0.64285714, 0.        , 0.01333333, 0.        ]], shape=(18162, 4)), 'product': array([[-2.70225982e+00],\n",
      "       [ 9.57649686e+00],\n",
      "       [ 2.78542325e+00],\n",
      "       ...,\n",
      "       [ 1.73981852e-01],\n",
      "       [ 1.75306113e+00],\n",
      "       [-2.07882331e+02]], shape=(18162, 1)), 'stddevy': array([[104.91133484]]), 'meany': array([[8.64215696]]), 'xoffset': array([ 0.        ,  0.        , -0.09459459,  0.        ]), 'xscale': array([7.        , 3.5       , 7.09459459, 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "ell1 = model.covar_module.base_kernel.kernels[0].covar_module_unbiased.lengthscale\n",
    "ell2 = model.covar_module.base_kernel.kernels[0].covar_module_unbiased.lengthscale\n",
    "p = model.covar_module.base_kernel.kernels[0].power\n",
    "oscale = model.covar_module.outputscale\n",
    "learned_mean = model.mean_module.constant\n",
    "train_x_trunc = torch.from_numpy(train_X_trunc)\n",
    "train_y_trunc = torch.from_numpy(train_Y_trunc)\n",
    "xoff = torch.min(train_x_trunc,0)[0]\n",
    "xscale = torch.max(train_x_trunc,0)[0] -torch.min(train_x_trunc,0)[0]\n",
    "train_x_nrm = (train_x_trunc - xoff)/xscale\n",
    "#covar_matrix = kernel(train_x_nrm, train_x_nrm,ell1,ell2,p,oscale)\n",
    "print(train_x_nrm)\n",
    "covar_matrix = model.covar_module(train_x_nrm,train_x_nrm).to_dense()\n",
    "covar_matrix = covar_matrix + model.likelihood.noise* torch.eye(covar_matrix.size(0))\n",
    "inverse_cov = torch.linalg.inv(covar_matrix)\n",
    "stdvs = train_y_trunc.std(dim=-2, keepdim=True)\n",
    "means = train_y_trunc.mean(dim=-2, keepdim=True)\n",
    "train_y_std = (train_y_trunc - means)/stdvs\n",
    "#prod = torch.matmul(inverse_cov,train_y_std - learned_mean).to_dense().detach().numpy()   #Store this\n",
    "prod = torch.linalg.solve(covar_matrix, train_y_std - learned_mean).to_dense().detach().numpy()\n",
    "print(prod)\n",
    "#Store all parameters in a dictionary\n",
    "my_dict = {\n",
    "    \"outputscale\": model.covar_module.outputscale.detach().numpy(),\n",
    "    \"theta1\": model.covar_module.base_kernel.kernels[0].covar_module_unbiased.lengthscale.detach().numpy(),\n",
    "    \"theta2\":model.covar_module.base_kernel.kernels[0].covar_module_biased.lengthscale.detach().numpy(),\n",
    "    \"learned_mean\": model.mean_module.constant.detach().numpy(),\n",
    "    \"power\": model.covar_module.base_kernel.kernels[0].power.detach().numpy(),\n",
    "    \"data\":train_x_nrm.numpy(),\n",
    "    \"product\": prod,\n",
    "    \"stddevy\": stdvs.detach().numpy(),\n",
    "    \"meany\": means.detach().numpy(),\n",
    "    \"xoffset\": xoff.detach().numpy(),\n",
    "    \"xscale\": xscale.detach().numpy()    \n",
    "}\n",
    "print(my_dict)\n",
    "#with open(\"data.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(my_dict, f)\n",
    "np.savez_compressed(\"data.npz\",outputscale= np.array([model.covar_module.outputscale.detach().numpy()]), theta1=model.covar_module.base_kernel.kernels[0].covar_module_unbiased.lengthscale.detach().numpy(),\n",
    "theta2 = model.covar_module.base_kernel.kernels[0].covar_module_biased.lengthscale.detach().numpy(),mean=np.array([model.mean_module.constant.detach().numpy()]),\n",
    "                   power = np.array([model.covar_module.base_kernel.kernels[0].power.detach().numpy()]), data=train_x_nrm.numpy(),product=prod,\n",
    "                   stddevy= stdvs.detach().numpy().flatten(), means=means.detach().numpy().flatten(), xoffset=xoff.detach().numpy(), xscale=xscale.detach().numpy())\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
