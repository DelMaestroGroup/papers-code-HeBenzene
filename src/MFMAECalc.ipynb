{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36287b89-0ac5-4c8a-a1e6-b258dfd139e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch as gpt\n",
    "import botorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP, SingleTaskGP\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.fit import fit_gpytorch_mll, fit_gpytorch_mll_torch\n",
    "from torch.optim import Adam\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import HeBz\n",
    "\n",
    "# Use CPU for this example\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa748e7-bd37-4fcf-952a-80a565b4ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_arr = []\n",
    "test_set_CC = []\n",
    "test_set_GP = []\n",
    "for seed in range(1,10):\n",
    "    #Load the data from file\n",
    "    V = np.load('../data/HeBz_sector_2475.npy')\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    z = np.array([])\n",
    "    data_points0 = []\n",
    "    Pot0 = np.array([])\n",
    "    for array in V:\n",
    "        x = np.append(x,array[0])\n",
    "        y = np.append(y,array[1])\n",
    "        z = np.append(z,array[2])\n",
    "        data_points0.append([array[0],array[1],array[2],1.0])\n",
    "        Pot0 = np.append(Pot0,array[3])\n",
    "    V2 = np.load('../data/new_pts_70.npy')\n",
    "    for array in V2:\n",
    "        x = np.append(x,array[0])\n",
    "        y = np.append(y,array[1])\n",
    "        z = np.append(z,array[2])\n",
    "        data_points0.append([array[0],array[1],array[2],1.0])\n",
    "        Pot0 = np.append(Pot0,array[3])\n",
    "    data_points0 = np.array(data_points0)\n",
    "    if seed in [1,2,7,8,9]:\n",
    "        print(seed)\n",
    "        data_points0 = data_points0[Pot0 <= 1000]\n",
    "        Pot0 = Pot0[Pot0 <= 1000]\n",
    "    # Remove 10% of the points\n",
    "    n = int(0.8*(data_points0.shape[0]))\n",
    "    test_data = []\n",
    "    rng1 = np.random.default_rng(seed)\n",
    "    index = rng1.choice(data_points0.shape[0], n, replace=False)\n",
    "    complement = np.delete(np.arange(data_points0.shape[0]), index)\n",
    "    data_points = data_points0[index]\n",
    "    Pot = Pot0[index]\n",
    "    data_eval = data_points0[complement]\n",
    "    correct_pot = Pot0[complement]\n",
    "    noise = 1e-6*np.ones_like(Pot)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Generate Source Task Data\n",
    "    # -----------------------------\n",
    "    DFTdata0 = np.load(\"../data/pbe0_113850_CP_D4_processed.npy\")\n",
    "    ind = np.where(np.isclose(DFTdata0[:,2],0.09459459))\n",
    "    refdata = DFTdata0[ind]\n",
    "    refdata[:,2] = -1*refdata[:,2]\n",
    "    DFTdata = np.concatenate((DFTdata0,refdata))\n",
    "    ind = np.argsort(DFTdata[:,2], kind='stable')\n",
    "    DFTdatazsrt = DFTdata[ind]\n",
    "    ind = np.argsort(DFTdatazsrt[:,1], kind='stable')\n",
    "    DFTdatayzsrt = DFTdatazsrt[ind]\n",
    "    DFTdataxrem = DFTdatayzsrt[::8]\n",
    "    ind = np.argsort(DFTdataxrem[:,0], kind='stable')\n",
    "    DFTdataxremzsrt = DFTdataxrem[ind]\n",
    "    #DFTdata2 = np.load(\"pbe0_corr_sutirtha_CP_D4_full_processed.npy\")\n",
    "    #DFTdatanew = np.concatenate((DFTdata2[:,0],DFTdata2[:,1],DFTdata2[:,2]),)\n",
    "    DFTdatazxrem = DFTdataxremzsrt\n",
    "    DFTdata2 = np.load(\"../data/pbe0_corr_sutirtha_CP_D4_full_processed.npy\")\n",
    "    DFTtotal = np.concatenate((DFTdatazxrem[:,0:3],DFTdata2[:,0:3]))\n",
    "    data_source = [[x, y, z, w] for x, y, z, w in zip(DFTtotal[:,0],DFTtotal[:,1],DFTtotal[:,2],np.zeros(len(DFTtotal)))]\n",
    "    m_source = np.concatenate((DFTdatazxrem[:,-1],DFTdata2[:,-1]))\n",
    "    #DFTtotal = np.concatenate((DFTdatazxrem[:,0:4],DFTdata2[:,0:4]))\n",
    "    #np.concatenate(DFTdatazxrem,[[\n",
    "    #print(np.unique(DFTdataxremzsrt[:,2]))\n",
    "    #print(np.unique(DFTdatazxrem[:,2]))\n",
    "    #data_source = [[x, y, z, w] for x, y, z, w in zip(DFTdatazxrem[:,0],DFTdatazxrem[:,1],DFTdatazxrem[:,2],np.zeros(len(DFTdatazxrem)))]\n",
    "    #m_source = DFTdatazxrem[:,-1]\n",
    "    noise_source = 1e-6*np.ones_like(m_source)\n",
    "    train_X = np.concatenate((data_points,data_source))\n",
    "    train_Y = np.concatenate((Pot,m_source))\n",
    "    train_Y = train_Y.reshape(-1,1)\n",
    "    train_Yvar = np.concatenate((noise,noise_source))\n",
    "    train_Yvar = train_Yvar.reshape(-1,1)\n",
    "    print(train_X.shape)\n",
    "    train_Y_trunc = train_Y[train_Y[:, 0]<=1000]\n",
    "    train_Yvar_trunc = train_Yvar[train_Y[:,0]<=1000]\n",
    "    train_X_trunc = train_X[train_Y[:, 0]<=1000, :]\n",
    "    print(train_X_trunc.shape)\n",
    "    \n",
    "    \n",
    "    # In[ ]:\n",
    "    modelname = '../data/MAE/my_model_multfidMAE' + str(seed) + '.pth'\n",
    "    state_dict = torch.load(modelname)\n",
    "    model = SingleTaskMultiFidelityGP(torch.from_numpy(train_X_trunc), torch.from_numpy(train_Y_trunc), torch.from_numpy(train_Yvar_trunc), data_fidelities=[len(train_X_trunc[0]) - 1], input_transform=Normalize(d=4), outcome_transform=Standardize(m=1))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        posterior = model.posterior(torch.from_numpy(np.array(data_eval)))\n",
    "        mean = posterior.mean\n",
    "        variance = posterior.variance\n",
    "    CCy = correct_pot.flatten()[correct_pot<=1000]\n",
    "    GPy = mean.numpy().flatten()[correct_pot<=1000]\n",
    "    abs_error = np.abs(correct_pot.flatten() - mean.numpy().flatten())[correct_pot<=1000]\n",
    "    for i in range(len(abs_error)):\n",
    "        test_set_CC.append(CCy[i])\n",
    "        test_set_GP.append(GPy[i])\n",
    "    rel_error = np.abs(np.divide(correct_pot.flatten() - mean.numpy().flatten(),correct_pot.flatten()))\n",
    "    print(np.mean(abs_error))\n",
    "    error_arr.append(np.mean(abs_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec9557-5c72-49fb-9741-48e3ff9ec154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(error_arr))) #8 seeds\n",
    "print(np.std(np.array(error_arr))/np.sqrt(8)) #8 seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a04a8-0263-4dff-b278-f66cb5ee77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GPCC.npy', np.array(test_set_CC))\n",
    "np.save('GPGP.npy', np.array(test_set_GP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9b8e6-7189-469a-977e-5fbb10339e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "plt.gcf().set_dpi(500)\n",
    "ax.plot(test_set_CC,test_set_GP,'bo')\n",
    "ax.plot(np.array(test_set_CC),np.array(test_set_CC),'k-')\n",
    "plt.savefig('Multifidelity-error.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37f297-5a0a-4fa6-bf34-42a699b109e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
